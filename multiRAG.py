#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šæ¨¡æ€RAGç³»ç»Ÿå°è£…ç±» - ç²¾ç®€ä¼˜åŒ–ç‰ˆ
æä¾›buildã€insertã€retrieveä¸‰ä¸ªæ ¸å¿ƒåŠŸèƒ½æ¥å£
æ”¯æŒcampuså’ŒpsychologyåŒåœºæ™¯
"""

import os
import json
import hashlib
import PyPDF2
from docx import Document
from typing import List, Dict, Any, Set
from pathlib import Path
import torch
from transformers import AutoModel
from sentence_transformers import SentenceTransformer, CrossEncoder

# å¯¼å…¥ç°æœ‰æ¨¡å—
from faiss_store_y import FAISSVectorStore
from Text_Processor.textsplitters import RecursiveCharacterTextSplitter
from Image_Processor.Image_Process import ImageExtractor


from Utils.Path import (
    CAMPUS_DOCS_DIR, PSYCHOLOGY_DOCS_DIR,
    CAMPUS_INDEX_DIR, PSYCHOLOGY_INDEX_DIR,
    CAMPUS_IMAGES_PATH, PSYCHOLOGY_IMAGES_PATH,
    CAMPUS_IMAGES_MAPPING_PATH, PSYCHOLOGY_IMAGES_MAPPING_PATH,
    CAMPUS_EXTRACTED_IMAGES_JSON, PSYCHOLOGY_EXTRACTED_IMAGES_JSON
)

class MultiRAG:
    """
    å¤šæ¨¡æ€RAGç³»ç»Ÿå°è£…ç±» 
    
    æ ¸å¿ƒåŠŸèƒ½:
    - build: å¯¹sourceæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶å»ºç«‹æ•°æ®åº“
    - insert: å¢é‡æ·»åŠ æ–‡ä»¶åˆ°çŸ¥è¯†åº“
    - retrieve: æ£€ç´¢ç›¸å…³åº¦æœ€é«˜çš„topkä¸ªç‰‡æ®µ
    """
    
    def __init__(self, 
                 scene: str = "campus",
                 embedding_model_path: str = "Qwen3-Embedding-0___6B",
                 cross_encoder_path: str = "cross-encoder-model"):
        """
        åˆå§‹åŒ–MultiRAGç³»ç»Ÿ
    
        Args:
            scene: åœºæ™¯ç±»å‹ ("campus" æˆ– "psychology")
            embedding_model_path: åµŒå…¥æ¨¡å‹è·¯å¾„
            cross_encoder_path: äº¤å‰ç¼–ç å™¨è·¯å¾„
        """
        # æ ¹æ®åœºæ™¯è®¾ç½®é»˜è®¤è·¯å¾„
        if scene == "campus":
            self.index_path = str(CAMPUS_INDEX_DIR)
            self.image_output_dir = str(CAMPUS_IMAGES_PATH)
            self.image_mapping_file = str(CAMPUS_IMAGES_MAPPING_PATH)
            self.collection_name = "campus_docs"
        elif scene == "psychology":
            self.index_path = str(PSYCHOLOGY_INDEX_DIR)
            self.image_output_dir = str(PSYCHOLOGY_IMAGES_PATH)
            self.image_mapping_file = str(PSYCHOLOGY_IMAGES_MAPPING_PATH)
            self.collection_name = "psychology_docs"
        else:
            self.index_path = str(CAMPUS_INDEX_DIR)
            self.image_output_dir = str(CAMPUS_IMAGES_PATH)
            self.image_mapping_file = str(CAMPUS_IMAGES_MAPPING_PATH)
            self.collection_name = f"{scene}_docs"
        
        self.scene = scene
        self.embedding_model_path = embedding_model_path
        self.cross_encoder_path = cross_encoder_path
        
        # åˆå§‹åŒ–æ¨¡å‹ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
        self._embedding_model = None
        self._cross_encoder = None
        self._vector_store = None
        self._text_splitter = None
        
        # åˆå§‹åŒ–å¤„ç†çŠ¶æ€è·Ÿè¸ª
        self._processed_files: Set[str] = set()
        self._processed_images: Set[str] = set()
        
        # ç¡®ä¿æ‰€æœ‰å¿…è¦çš„ç›®å½•å­˜åœ¨
        self._ensure_directories()
        
        # åˆå§‹åŒ–å¿…è¦çš„æ–‡ä»¶
        self._initialize_files()
            
        print(f"MultiRAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ - {self.scene}åœºæ™¯")
        print(f"ç´¢å¼•è·¯å¾„: {self.index_path}")
        print(f"å›¾ç‰‡è¾“å‡ºç›®å½•: {self.image_output_dir}")

    ###########################################################################
    # åˆå§‹åŒ–ç›¸å…³æ–¹æ³•
    ###########################################################################
    
    def _ensure_directories(self):
        """ç¡®ä¿æ‰€æœ‰å¿…è¦çš„ç›®å½•å­˜åœ¨"""
        directories = [
            self.index_path,
            self.image_output_dir,
            os.path.dirname(self.image_mapping_file) if os.path.dirname(self.image_mapping_file) else None
        ]
        
        for directory in directories:
            if directory and directory != ".":
                os.makedirs(directory, exist_ok=True)
    
    def _initialize_files(self):
        """åˆå§‹åŒ–å¿…è¦çš„æ–‡ä»¶"""
        # åˆå§‹åŒ–å›¾ç‰‡æ˜ å°„æ–‡ä»¶
        if not os.path.exists(self.image_mapping_file):
            with open(self.image_mapping_file, 'w', encoding='utf-8') as f:
                json.dump({}, f, ensure_ascii=False, indent=2)
        
        # åŠ è½½ç°æœ‰çš„å›¾ç‰‡æ˜ å°„
        self._load_image_mapping()
        
        # åŠ è½½å·²å¤„ç†çš„æ–‡ä»¶è®°å½•
        self._load_processed_files()

    def _load_processed_files(self):
        """åŠ è½½å·²å¤„ç†çš„æ–‡ä»¶è®°å½•"""
        processed_files_file = os.path.join(self.index_path, f"processed_files_{self.scene}.json")
        if os.path.exists(processed_files_file):
            try:
                with open(processed_files_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self._processed_files = set(data.get('files', []))
                    self._processed_images = set(data.get('images', []))
            except:
                self._processed_files = set()
                self._processed_images = set()

    def _save_processed_files(self):
        """ä¿å­˜å·²å¤„ç†çš„æ–‡ä»¶è®°å½•"""
        processed_files_file = os.path.join(self.index_path, f"processed_files_{self.scene}.json")
        try:
            data = {
                'files': list(self._processed_files),
                'images': list(self._processed_images)
            }
            with open(processed_files_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ä¿å­˜å·²å¤„ç†æ–‡ä»¶è®°å½•å¤±è´¥: {e}")

    def _get_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼ç”¨äºå»é‡"""
        try:
            with open(file_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except:
            return os.path.basename(file_path)  # å›é€€åˆ°æ–‡ä»¶å

    ###########################################################################
    # æ¨¡å‹åŠ è½½å±æ€§ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
    ###########################################################################
    
    @property
    def embedding_model(self):
        """åµŒå…¥æ¨¡å‹å±æ€§ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
        if self._embedding_model is None:
            try:
                self._embedding_model = SentenceTransformer(
                    self.embedding_model_path,
                    trust_remote_code=True
                )
            except Exception as e:
                print(f"åŠ è½½åµŒå…¥æ¨¡å‹å¤±è´¥: {e}")
                raise
        return self._embedding_model

    @property
    def vector_store(self):
        """å‘é‡å­˜å‚¨å±æ€§ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
        if self._vector_store is None:
            try:
                self._vector_store = FAISSVectorStore(index_path=self.index_path)
            except Exception as e:
                print(f"åŠ è½½å‘é‡å­˜å‚¨å¤±è´¥: {e}")
                raise
        return self._vector_store

    @property
    def text_splitter(self):
        """æ–‡æœ¬åˆ†å‰²å™¨å±æ€§ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
        if self._text_splitter is None:
            try:
                self._text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=500,
                    chunk_overlap=50,
                    length_function=len,
                )
            except Exception as e:
                print(f"åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨å¤±è´¥: {e}")
                raise
        return self._text_splitter
    
    ###########################################################################
    # æ ¸å¿ƒæ„å»ºæ–¹æ³•
    ###########################################################################
    
    def build(self, source: str):
        """
        å¯¹sourceæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶å»ºç«‹æ•°æ®åº“
        """
        if not os.path.isdir(source):
            raise NotADirectoryError(f"æ–‡ä»¶å¤¹ {source} ä¸å­˜åœ¨æˆ–ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ–‡ä»¶å¤¹")
        
        print(f"=== å¼€å§‹æ„å»º{self.scene}åœºæ™¯å¤šæ¨¡æ€RAGæ•°æ®åº“ ===")
        
        # ç¡®ä¿æ‰€æœ‰å¿…è¦çš„ç›®å½•å’Œæ–‡ä»¶å­˜åœ¨
        self._ensure_directories()
        self._initialize_files()
        
        # é‡ç½®å‘é‡å­˜å‚¨
        self._vector_store = FAISSVectorStore(
            index_path=self.index_path,
            collection_name=self.collection_name,
            dimension=1024,
            reset=True
        )
        
        # æ¸…ç©ºå¤„ç†è®°å½•
        self._processed_files.clear()
        self._processed_images.clear()
        
        # 1. å¤„ç†æ–‡æœ¬æ–‡æ¡£
        print("\næ­¥éª¤1: å¤„ç†æ–‡æœ¬æ–‡æ¡£...")
        self._process_text_documents(source)
        
        # 2. å¤„ç†å›¾ç‰‡
        print("\næ­¥éª¤2: å¤„ç†å›¾ç‰‡...")
        processed_images = self._process_images(source)
        
        # 3. å°†å›¾ç‰‡æè¿°æ·»åŠ åˆ°æ•°æ®åº“
        if processed_images:
            print("\næ­¥éª¤3: å°†å›¾ç‰‡æè¿°æ·»åŠ åˆ°æ•°æ®åº“...")
            self._add_images_to_database(processed_images)
        
        # ä¿å­˜å¤„ç†è®°å½•
        self._save_processed_files()
        
        print(f"\n=== {self.scene}åœºæ™¯æ•°æ®åº“æ„å»ºå®Œæˆ ===")
        self._print_database_stats()
    
    def insert(self, source: str):
        """
        å°†sourceæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶åŠ å…¥çŸ¥è¯†åº“ï¼ˆå¢é‡æ·»åŠ ï¼‰
        """
        if not os.path.isdir(source):
            raise NotADirectoryError(f"æ–‡ä»¶å¤¹ {source} ä¸å­˜åœ¨æˆ–ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ–‡ä»¶å¤¹")
        
        print(f"=== å¼€å§‹å¢é‡æ·»åŠ æ–‡æ¡£åˆ°{self.scene}åœºæ™¯æ•°æ®åº“ ===")
        
        # ç¡®ä¿æ‰€æœ‰å¿…è¦çš„ç›®å½•å’Œæ–‡ä»¶å­˜åœ¨
        self._ensure_directories()
        self._initialize_files()
        
        # 1. å¤„ç†æ–‡æœ¬æ–‡æ¡£ï¼ˆå¢é‡æ·»åŠ ï¼‰
        print("\næ­¥éª¤1: å¢é‡æ·»åŠ æ–‡æœ¬æ–‡æ¡£...")
        new_text_files = self._process_text_documents(source, incremental=True)
        
        # 2. å¤„ç†å›¾ç‰‡ï¼ˆå¢é‡æ·»åŠ ï¼‰
        print("\næ­¥éª¤2: å¤„ç†æ–°å›¾ç‰‡...")
        processed_images = self._process_images(source, incremental=True)
        
        # 3. å°†å›¾ç‰‡æè¿°æ·»åŠ åˆ°æ•°æ®åº“
        if processed_images:
            print("\næ­¥éª¤3: å°†æ–°å›¾ç‰‡æè¿°æ·»åŠ åˆ°æ•°æ®åº“...")
            self._add_images_to_database(processed_images, incremental=True)
        
        # 4. ä¿å­˜å¤„ç†è®°å½•
        self._save_processed_files()
        
        print(f"\n=== {self.scene}åœºæ™¯æ–‡æ¡£å¢é‡æ·»åŠ å®Œæˆ ===")
        print(f"æ–°å¢æ–‡æœ¬æ–‡ä»¶: {new_text_files} ä¸ª")
        print(f"æ–°å¢å›¾ç‰‡: {len(processed_images)} ä¸ª")
        self._print_database_stats()

    ###########################################################################
    # æ–‡æœ¬å¤„ç†ç›¸å…³æ–¹æ³•
    ###########################################################################
    
    def _process_text_documents(self, source_folder: str, incremental: bool = False) -> int:
        """å¤„ç†æ–‡æœ¬æ–‡æ¡£"""
        # è·å–æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
        supported_extensions = ['.txt', '.md', '.markdown', '.pdf', '.docx']
        all_files = []
    
        for root, dirs, files in os.walk(source_folder):
            for file in files:
                file_path = os.path.join(root, file)
            
                # è·³è¿‡ä¸´æ—¶æ–‡ä»¶å’Œç³»ç»Ÿæ–‡ä»¶
                if self._should_skip_file(file):
                    continue
            
                if any(file.lower().endswith(ext) for ext in supported_extensions):
                    all_files.append(file_path)
    
        if not all_files:
            print("æ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„æ–‡æœ¬æ–‡ä»¶")
            return 0
        
        print(f"æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡æœ¬æ–‡ä»¶")
        
        new_files = 0
    
        for file_idx, file_path in enumerate(all_files, 1):
            try:
                # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡ï¼ˆå¢é‡æ¨¡å¼ï¼‰
                file_hash = self._get_file_hash(file_path)
                if incremental and file_hash in self._processed_files:
                    continue
            
                print(f"å¤„ç†ç¬¬ {file_idx}/{len(all_files)} ä¸ªæ–‡ä»¶: {os.path.basename(file_path)}")
            
                # åˆ†å‰²æ–‡æ¡£
                chunks = self._split_document(file_path)
                if not chunks:
                    continue
            
                # ç”ŸæˆåµŒå…¥å‘é‡
                embeddings = self.embedding_model.encode(chunks)
            
                # å‡†å¤‡æ•°æ®
                documents = []
                embeddings_list = []
                ids = []
            
                for chunk_idx, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
                    unique_id = self._generate_text_chunk_id(file_hash, chunk_idx)
                    documents.append(chunk)
                    embeddings_list.append(embedding.tolist())
                    ids.append(unique_id)
            
                # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
                self.vector_store.add(
                    documents=documents,
                    embeddings=embeddings_list,
                    ids=ids
                )
            
                # è®°å½•å·²å¤„ç†æ–‡ä»¶
                self._processed_files.add(file_hash)
                new_files += 1
            
            except Exception as e:
                print(f"å¤„ç†æ–‡ä»¶ {os.path.basename(file_path)} æ—¶å‡ºé”™: {e}")
                continue
    
        return new_files

    def _split_document(self, filename: str) -> List[str]:
        """åˆ†å‰²æ–‡æ¡£ä¸ºchunks"""
        content = self._read_file(filename)
        if not content.strip():
            return []
        
        return self.text_splitter.split_text(content)

    def _read_file(self, filename: str) -> str:
        """è¯»å–æ–‡ä»¶å†…å®¹ï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
        if filename.endswith('.txt') or filename.endswith('.md') or filename.endswith('.markdown'):
            try:
                with open(filename, 'r', encoding='utf-8') as file:
                    return file.read()
            except UnicodeDecodeError:
                try:
                    with open(filename, 'r', encoding='gbk') as file:
                        return file.read()
                except:
                    return ""
        
        elif filename.endswith('.pdf'):
            try:
                text = ""
                with open(filename, 'rb') as file:
                    reader = PyPDF2.PdfReader(file)
                    for page in reader.pages:
                        text += page.extract_text() or ""
                return text
            except:
                return ""
        
        elif filename.endswith('.docx'):
            try:
                doc = Document(filename)
                return "\n".join([para.text for para in doc.paragraphs])
            except:
                return ""
        
        else:
            return ""

    def _should_skip_file(self, filename: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è·³è¿‡æ–‡ä»¶"""
        # è·³è¿‡Wordä¸´æ—¶æ–‡ä»¶
        if filename.startswith('~$'):
            return True
    
        # è·³è¿‡ç³»ç»Ÿéšè—æ–‡ä»¶
        if filename.startswith('.'):
            return True
    
        # è·³è¿‡ç³»ç»Ÿæ–‡ä»¶
        system_files = ['Thumbs.db', '.DS_Store']
        if filename in system_files:
            return True
    
        return False

    def _generate_text_chunk_id(self, file_hash: str, chunk_index: int) -> str:
        """ç”Ÿæˆæ–‡æœ¬chunk ID"""
        return f"text_{self.scene}_{file_hash}_chunk_{chunk_index}"

    ###########################################################################
    # å›¾ç‰‡å¤„ç†ç›¸å…³æ–¹æ³•
    ###########################################################################
    
    def _process_images(self, source_folder: str, incremental: bool = False) -> List[Dict]:
        """å¤„ç†å›¾ç‰‡"""
        # ä½¿ç”¨ç°æœ‰çš„ImageExtractor
        extractor = ImageExtractor(source_folder, output_dir=self.image_output_dir)
        processed_data = extractor.process_all_documents()

        if not processed_data:
            return []

        # ç›´æ¥ä½¿ç”¨ImageExtractorå¤„ç†çš„ç»“æœ
        saved_images = []
        for img_data in processed_data:
            try:
                # è·å–å›¾ç‰‡å“ˆå¸Œ
                image_hash = img_data.get('image_hash', 
                                    hashlib.md5(img_data.get('image_data', b'')).hexdigest()[:16])
            
                # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡ï¼ˆå¢é‡æ¨¡å¼ï¼‰
                if incremental and image_hash in self._processed_images:
                    continue
            
                # æ›´æ–°å›¾ç‰‡æ•°æ®
                image_path = img_data.get('image_path', '')
                image_filename = img_data.get('image_filename', '')
            
                if not image_path and image_filename:
                    image_path = os.path.join(self.image_output_dir, image_filename)
            
                updated_img_data = img_data.copy()
                updated_img_data.update({
                    'image_path': image_path,
                    'image_filename': image_filename,
                    'image_hash': image_hash
                })
            
                saved_images.append(updated_img_data)
                self._processed_images.add(image_hash)
            
            except:
                continue

        return saved_images

    def _load_image_mapping(self) -> Dict:
        """åŠ è½½å›¾ç‰‡æ˜ å°„æ–‡ä»¶"""
        try:
            if os.path.exists(self.image_mapping_file):
                with open(self.image_mapping_file, 'r', encoding='utf-8') as f:
                    mapping = json.load(f)
                    print(f"âœ… æˆåŠŸåŠ è½½å›¾ç‰‡æ˜ å°„æ–‡ä»¶ï¼ŒåŒ…å« {len(mapping)} ä¸ªå›¾ç‰‡æ¡ç›®")
                    return mapping
            else:
                print(f"âŒ å›¾ç‰‡æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨: {self.image_mapping_file}")
                return {}
        except Exception as e:
            print(f"âŒ åŠ è½½å›¾ç‰‡æ˜ å°„æ–‡ä»¶å¤±è´¥: {e}")
            return {}
        
    def _save_image_mapping(self):
        """ä¿å­˜å›¾ç‰‡æ˜ å°„æ–‡ä»¶"""
        try:
            with open(self.image_mapping_file, 'w', encoding='utf-8') as f:
                json.dump(self._image_mapping, f, ensure_ascii=False, indent=2)
        except:
            pass

    def _add_images_to_database(self, processed_images: List[Dict], incremental: bool = False):
        """å°†å›¾ç‰‡æè¿°æ·»åŠ åˆ°æ•°æ®åº“"""
        if not processed_images:
            return
    
        # åˆ›å»ºå›¾ç‰‡chunkså’Œæ˜ å°„
        image_chunks = []
        image_mapping = {}
    
        for img_data in processed_images:
            # ä½¿ç”¨ç»Ÿä¸€çš„å›¾ç‰‡IDç”Ÿæˆæ–¹æ³•
            image_hash = img_data.get('image_hash', '')
            image_id = self._generate_image_id(image_hash)
        
            # åœ¨æè¿°å‰åŠ ä¸Šimage x:æ ‡ç­¾
            chunk_content = f"{image_id}: {img_data['enhanced_description']}"
        
            # å­˜å‚¨å›¾ç‰‡æ˜ å°„ä¿¡æ¯
            image_mapping[image_id] = {
                'image_path': img_data.get('image_path', ''),
                'image_filename': img_data.get('image_filename', ''),
                'source_file': img_data['source_file'],
                'enhanced_description': img_data['enhanced_description'],
                'image_hash': image_hash
            }
        
            # åˆ›å»ºchunk
            chunk = {
                'content': chunk_content,
                'chunk_id': image_id
            }
        
            image_chunks.append(chunk)
    
        # æ›´æ–°å›¾ç‰‡æ˜ å°„æ–‡ä»¶
        if incremental:
            self._image_mapping.update(image_mapping)
        else:
            self._image_mapping = image_mapping
        
        self._save_image_mapping()
    
        # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
        for chunk in image_chunks:
            try:
                # ç”Ÿæˆembedding
                embedding = self.embedding_model.encode([chunk['content']])[0]
                
                # æ·»åŠ åˆ°FAISSå­˜å‚¨
                self.vector_store.add(
                    documents=[chunk['content']],
                    embeddings=[embedding.tolist()],
                    ids=[chunk['chunk_id']]
                )
            except:
                continue

    def _generate_image_id(self, image_hash: str) -> str:
        """ç”Ÿæˆå›¾ç‰‡ID"""
        return f"image_{self.scene}_{image_hash}"

    ###########################################################################
    # æ£€ç´¢æ–¹æ³•
    ###########################################################################
    
    def retrieve(self, query: str, topk: int = 5) -> List[Dict[str, Any]]:
        """
        ä¿®å¤çš„æ£€ç´¢æ–¹æ³• - æ­£ç¡®ä½¿ç”¨å›¾ç‰‡æ˜ å°„æ–‡ä»¶
        """
        try:
            # 1. ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = self.embedding_model.encode(
                query,
                prompt_name="query", 
                convert_to_tensor=False,
                normalize_embeddings=True
            ).tolist()

            # 2. ä»FAISSæ£€ç´¢
            results = self.vector_store.search(query_embedding, topk)
            
            if not results:
                return []

            # 3. åŠ è½½å›¾ç‰‡æ˜ å°„æ–‡ä»¶
            image_mapping = self._load_image_mapping()
            
            # 4. æ ¼å¼åŒ–è¾“å‡ºå¹¶å…³è”å›¾ç‰‡ä¿¡æ¯
            formatted_results = []
            for result in results:
                content = result.get('content', '')
                score = result.get('score', 0)
                
                # ã€å…³é”®ä¿®å¤ã€‘æ£€æŸ¥æ˜¯å¦æ˜¯å›¾ç‰‡æè¿°
                # å›¾ç‰‡æè¿°é€šå¸¸ä»¥ "image_" å¼€å¤´ï¼Œåé¢è·Ÿç€åœºæ™¯å’Œå“ˆå¸Œå€¼
                if content.startswith('image_'):
                    # æå–å›¾ç‰‡IDï¼ˆæ ¼å¼ï¼šimage_psychology_xxxxï¼‰
                    image_id = content.split(':', 1)[0].strip() if ':' in content else content.strip()
                    
                    print(f"ğŸ” å‘ç°å›¾ç‰‡å†…å®¹: {image_id}")
                    
                    # ä»æ˜ å°„æ–‡ä»¶ä¸­è·å–å›¾ç‰‡ä¿¡æ¯
                    img_info = image_mapping.get(image_id, {})
                    img_path = img_info.get('image_path', '')
                    description = img_info.get('enhanced_description', img_info.get('ai_description', ''))
                    
                    # éªŒè¯å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                    if img_path and os.path.exists(img_path):
                        print(f"âœ… å›¾ç‰‡æ–‡ä»¶å­˜åœ¨: {img_path}")
                        formatted_results.append({
                            "type": 1,  # å›¾ç‰‡ç±»å‹
                            "document": description,
                            "source": img_path,
                            "score": score,
                            "content": content
                        })
                    else:
                        print(f"âŒ å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {img_path}")
                        # å³ä½¿æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä¹Ÿè¿”å›å›¾ç‰‡æè¿°
                        formatted_results.append({
                            "type": 1,
                            "document": content,
                            "source": "",
                            "score": score,
                            "content": content
                        })
                else:
                    # çº¯æ–‡æœ¬
                    formatted_results.append({
                        "type": 0,  # æ–‡æœ¬ç±»å‹
                        "document": content,
                        "source": "",
                        "score": score,
                        "content": content
                    })

            # æ‰“å°è°ƒè¯•ä¿¡æ¯
            image_count = len([r for r in formatted_results if r['type'] == 1])
            text_count = len([r for r in formatted_results if r['type'] == 0])
            print(f"ğŸ“Š æ£€ç´¢ç»“æœç»Ÿè®¡: {image_count} ä¸ªå›¾ç‰‡, {text_count} ä¸ªæ–‡æœ¬")
            
            return formatted_results
            
        except Exception as e:
            print(f"æ£€ç´¢è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return []

    
    ###########################################################################
    # è¾…åŠ©æ–¹æ³•
    ###########################################################################
    
    def _print_database_stats(self):
        """æ‰“å°æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        total_docs = self.vector_store.count()
    
        # ç»Ÿè®¡å›¾ç‰‡å’Œæ–‡æœ¬æ–‡æ¡£æ•°é‡
        image_count = 0
        text_count = 0
    
        for doc_id in self.vector_store.ids:
            if doc_id.startswith(f'image_{self.scene}_'):
                image_count += 1
            else:
                text_count += 1
    
        print(f"\n{self.scene}åœºæ™¯æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  æ€»æ–‡æ¡£æ•°: {total_docs}")
        print(f"  æ–‡æœ¬ç‰‡æ®µæ•°: {text_count}")
        print(f"  å›¾ç‰‡æè¿°æ•°: {image_count}")
        print(f"  å·²å¤„ç†æ–‡ä»¶: {len(self._processed_files)}")
        print(f"  å·²å¤„ç†å›¾ç‰‡: {len(self._processed_images)}")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # 1. åˆ›å»º MultiRAG å®ä¾‹
    campus_rag = MultiRAG(scene="campus")
    psychology_rag = MultiRAG(scene="psychology")

    # 2. æ„å»ºæ•°æ®åº“
    campus_rag.build(str(CAMPUS_DOCS_DIR))
    psychology_rag.build(str(PSYCHOLOGY_DOCS_DIR))
    
    # 3. æµ‹è¯•æ£€ç´¢åŠŸèƒ½
    campus_results = campus_rag.retrieve("æ ¡å›­é‚®ç®±å¦‚ä½•ä½¿ç”¨", 5)
    print(f"æ ¡å›­åœºæ™¯æ£€ç´¢ç»“æœ: {len(campus_results)} ä¸ª")
    
    psychology_results = psychology_rag.retrieve("å¦‚ä½•ç¼“è§£ç„¦è™‘", 5)
    print(f"å¿ƒç†å­¦åœºæ™¯æ£€ç´¢ç»“æœ: {len(psychology_results)} ä¸ª")
    
    # 4. å¢é‡æ·»åŠ æ–‡æ¡£
    campus_rag.insert(str(CAMPUS_DOCS_DIR))
    psychology_rag.insert(str(PSYCHOLOGY_DOCS_DIR))